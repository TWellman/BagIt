{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate BagIt Archives for OBIS-USA Data\n",
    "\n",
    "#### Tristan P. Wellman<br>Science Analytics and Synthesis (SAS)<br>U.S. Geological Survey, Denver, Colorado\n",
    "#### last modified 2/17/2019\n",
    "\n",
    "Script creates BagIt archives for the OBIS-USA ScienceBase Collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python packages\n",
    "#\n",
    "import os\n",
    "import re\n",
    "import tarfile\n",
    "import bagit\n",
    "import tempfile\n",
    "import shutil\n",
    "import json\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import requests\n",
    "import uuid\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition to set-up optional archive arguments \n",
    "#\n",
    "# Functions:\n",
    "# 1) Specify archive metadata, and \n",
    "# 2) Specify search criteria to select data files from ScienceBase items. \n",
    "# 3) customize defaults inputs via kwargs \n",
    "\n",
    "def archive_options(sbitem, **kwargs):\n",
    "\n",
    "    archive_func = collections.OrderedDict()\n",
    "        \n",
    "    # Archive metadata (defaults) -\n",
    "    #\n",
    "    archive_func['archive_meta'] = collections.OrderedDict([\n",
    "        ('Archive-Tag-Name', 'Archive of ScienceBase Item'),\n",
    "        ('Archive-Prcessing-Date', datetime.datetime.now().isoformat()),\n",
    "        ('Archive-Host-Machine', str(uuid.uuid1())),\n",
    "        ('Archive-Job-Number', str(uuid.uuid4())),\n",
    "        ('Source-Agency-Name', 'United States Geological Survey'),\n",
    "        ('Source-Agency-Physical-Address', 'Denver Federal Center, Building 810, Lakewood, Colorado, USA'),\n",
    "        ('Source-Agency-Group', 'Science Analytics and Synthesis (SAS), Core Science Systems'),\n",
    "        ('Source-Agency-Contact-Name', 'John Doe'),\n",
    "        ('Source-Agency-Contact-Phone', '999-999-9999'),\n",
    "        ('Source-Agency-Contact-Email', 'jdoe@usgs.gov'),\n",
    "        ('Source-Agency-Data-Source', sbitem['link']['url']),\n",
    "        ('Source-Agency-Data-Title',sbitem['title']),\n",
    "    ])\n",
    "    \n",
    "#   Search criteria - \n",
    "#\n",
    "#   Inputs: \"include\" and \"exclude\" keys with search parameters. \n",
    "#          first key (include or exclude) is performed first, \n",
    "#          second key is performed second\n",
    "# \n",
    "#   function: selects files to include and/or exclude using search parameters\n",
    "#\n",
    "#   Search Parameters\n",
    "#       1) 'ignore': do not use  \n",
    "#       2) 'all': selects all files \n",
    "#       3) custom (text, regex) search term  e.g. '\\.nc' selects files with .nc extension\n",
    "\n",
    "#   Include all item files except (exclude) those with .nc* file extensions \n",
    "#\n",
    "    archive_func['search'] = collections.OrderedDict([('include' ,'all'), ('exclude' , '\\.nc')])\n",
    "    \n",
    "    \n",
    "#   Overwrite or add key-values via **kwargs (optional) \n",
    "\n",
    "    if 'custom_meta' in kwargs:\n",
    "        for key in kwargs['custom_meta']:\n",
    "            if key in archive_func:\n",
    "                for sub_key in kwargs['custom_meta'][key]:\n",
    "                    archive_func[key][sub_key] = kwargs['custom_meta'][key][sub_key]\n",
    "            else:\n",
    "                archive_func[key] = kwargs['custom_meta'][key]\n",
    "            \n",
    "\n",
    "    return archive_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python class to archive ScienceBase item using BagIt\n",
    "#\n",
    "class archive_sbitem():\n",
    "    \n",
    "    '''Class to archive ScienceBase data by item number.\n",
    "       Functions include: select, retrieve, describe, package, \n",
    "       and compress archive content'''\n",
    "    \n",
    "    \n",
    "    # Store ScienceBase item information\n",
    "    #\n",
    "    def __init__(self, sbitem, **kwargs):\n",
    "\n",
    "        '''Performs all processing steps in sequence to create BagIt archive'''\n",
    "            \n",
    "        # task sequence (workflow)    \n",
    "        #\n",
    "        self.sbitem = sbitem\n",
    "        self._inputs(**kwargs)\n",
    "        self._gen_archive()\n",
    "        self._sbfile_select()\n",
    "        self._get_datafiles()\n",
    "        self._add_meta()\n",
    "        self._validate()\n",
    "        self._tar_archive()\n",
    "        \n",
    "        return print(\"\\t{}: {}\".format('Archive completed for ScienceBase item: ', sbitem['id']))\n",
    "        \n",
    "        \n",
    "    # Access get_item capabilities\n",
    "    #\n",
    "    def __getitem__(self, item):\n",
    "        return getattr(self, item)\n",
    "     \n",
    "        \n",
    "    # Create Bagit object and temporary workspace \n",
    "    #\n",
    "    def _gen_archive(self):\n",
    "        \n",
    "        '''Structures BagIt folder object'''\n",
    "        \n",
    "        self.bagit_folder = tempfile.mkdtemp()\n",
    "        self.data_folder = os.path.join(self.bagit_folder, 'data')\n",
    "        self.archive = bagit.make_bag(self.bagit_folder, checksum=['sha256'])\n",
    "        \n",
    "        \n",
    "    # Register search criteria (actual functions) and archive metadata\n",
    "    #\n",
    "    def _inputs(self, **kwargs):\n",
    "        \n",
    "        '''Stores input search criteria and archive metadata'''\n",
    "        \n",
    "        # search criteria (include, exclude tags)\n",
    "        #\n",
    "        if 'search' not in kwargs:\n",
    "            self.search = {'include' :'all', 'exclude' : None}\n",
    "        elif 'include' not in kwargs['search'] or 'exclude' not in kwargs['search']:\n",
    "            self.search = {'include' :'all', 'exclude' : None}\n",
    "        else:\n",
    "            self.search = kwargs['search']\n",
    "            \n",
    "        if isinstance(self.search['include'], str):\n",
    "            self.search['include'] = [self.search['include']]\n",
    "        if isinstance(self.search['exclude'], str):\n",
    "            self.search['exclude'] = [self.search['exclude']]\n",
    "            \n",
    "        for key in self.search:\n",
    "            parlist = []\n",
    "            for criteria in self.search[key]:\n",
    "                if criteria.lower() == 'all':\n",
    "                    parlist.append(\"(.*?)\")\n",
    "                elif criteria.lower() == 'ignore':\n",
    "                    parlist.append(None)\n",
    "                else:\n",
    "                    parlist.append(criteria)\n",
    "            self.search.update({key:parlist})\n",
    "                        \n",
    "        # archive metadata record\n",
    "        #\n",
    "        if 'archive_meta' not in kwargs:\n",
    "            self.archive_meta = None\n",
    "        else:\n",
    "            self.archive_meta = kwargs['archive_meta']\n",
    "    \n",
    "        \n",
    "    # Select ScienceBase file content using quasi-flexible search criteria\n",
    "    #\n",
    "    def _sbfile_select(self):\n",
    "    \n",
    "        '''Identifies ScienceBase files to archive based on search criteria'''\n",
    "            \n",
    "        cdict = {'include': True, 'exclude' : False}\n",
    "         \n",
    "        # search through ScienceBase item (files and facets keywords)\n",
    "        #\n",
    "        self.file_select = []\n",
    "        self.file_name = []\n",
    "        if 'facets' in self.sbitem:\n",
    "            for fdic in self.sbitem['facets']:\n",
    "                if 'files' in fdic:\n",
    "                    for dfile in fdic['files']:\n",
    "                        file_chk = None\n",
    "                        for item in self.search.items():\n",
    "                            fchk = cdict[item[0].lower()]\n",
    "                            if item[1] is not None:\n",
    "                                for criteria in item[1]:\n",
    "                                    regx_srch = r\"{}\".format(criteria)\n",
    "                                    if re.search(regx_srch, dfile['name']):\n",
    "                                        file_chk = fchk                \n",
    "                        if file_chk:\n",
    "                            self.file_select.append(dfile['downloadUri'])\n",
    "                            self.file_name.append(dfile['name'])\n",
    "                        \n",
    "        if 'files' in self.sbitem:\n",
    "            for dfile in self.sbitem['files']:         \n",
    "                file_chk = None\n",
    "                for item in self.search.items():\n",
    "                    fchk = cdict[item[0].lower()]\n",
    "                    if item[1] is not None:\n",
    "                        for criteria in item[1]:\n",
    "                            regx_srch = r\"{}\".format(criteria)\n",
    "                            if re.search(regx_srch, dfile['name']):\n",
    "                                file_chk = fchk            \n",
    "                if file_chk:\n",
    "                    self.file_select.append(dfile['downloadUri'])\n",
    "                    self.file_name.append(dfile['name'])\n",
    "    \n",
    "            \n",
    "    # stream file retrieve, file insertion into archive folder \n",
    "    #\n",
    "    def _get_datafiles(self):\n",
    "        \n",
    "        '''Streams ScienceBase files into BagIt data folder'''\n",
    "        \n",
    "        for indx, file_path in enumerate(self.file_select):\n",
    "            request = requests.get(file_path, stream=True)\n",
    "            if request.status_code == 200:\n",
    "                bag_path = self.data_folder + '/' + self.file_name[indx] \n",
    "                with open(bag_path, 'wb') as f:\n",
    "                    request.raw.decode_content = True\n",
    "                    shutil.copyfileobj(request.raw, f)            \n",
    "        sbitem_fname  = self.data_folder + '/' + 'ScienceBase_record_' + self.sbitem['id'] + '.json'\n",
    "        with open(sbitem_fname, 'w') as f:\n",
    "            json.dump(self.sbitem, f)   \n",
    "        self.archive.save(manifests=True, processes=3)\n",
    "    \n",
    "    \n",
    "    # Customize archive metadata  \n",
    "    #\n",
    "    def _add_meta(self):\n",
    "        \n",
    "        '''Adds metadata to BagIt folder'''\n",
    "        \n",
    "        if self.archive_meta:\n",
    "            self.archive.info.update(self.archive_meta)\n",
    "            self.archive.save(manifests=True, processes=4)  \n",
    "                \n",
    "                \n",
    "    # Validate archive\n",
    "    #\n",
    "    def _validate(self):\n",
    "        \n",
    "        '''validation check for BagIt folder'''\n",
    "        \n",
    "        self.validate = []\n",
    "        if self.archive.is_valid():\n",
    "            self.validate.append(\"Bagit archive is structurally valid\")\n",
    "        else:\n",
    "            self.validate.append(\"Bagit archive is structurally invalid\")\n",
    "        try:\n",
    "            self.archive.validate()\n",
    "        except bagit.BagValidationError as e:\n",
    "            for d in e.details:\n",
    "                if isinstance(d, bagit.ChecksumMismatch):\n",
    "                    self.validate.append(\"expected %s to have %s checksum of %s but found %s\" %\n",
    "                          (d.path, d.algorithm, d.expected, d.found))\n",
    "                    \n",
    "                    \n",
    "    # Package archive folder in *.tar compressed format (save to local archive_folder)\n",
    "    #\n",
    "    def _tar_archive(self):\n",
    "        \n",
    "        '''Compresses BagIt folder and save to archive folder'''\n",
    "        \n",
    "        # Ensure archive folder exists\n",
    "        #\n",
    "        dirname = 'Archives'\n",
    "        tar_directory = os.path.join(os.getcwd(),dirname)\n",
    "        if not os.path.isdir(tar_directory):  \n",
    "            try:  \n",
    "                os.mkdir(tar_directory)\n",
    "            except OSError: \n",
    "                print(\"Creation of the directory %s failed\" % tar_directory)\n",
    "\n",
    "        # Save archive as *.tar file\n",
    "        #\n",
    "        #sbitem_title = self.sbitem['title']   \n",
    "        #tar_filename = (re.sub(r'\\W+', '', sbitem_title).replace(' ','')) + '.tgz'\n",
    "        tar_filename = 'ScienceBase_Archive_' + self.sbitem['id'] + '.tgz'\n",
    "        with tarfile.open('./' + dirname + '/' + tar_filename, \"w:gz\") as tar:\n",
    "            tar.add(self.bagit_folder, arcname=tar_filename.strip('.tgz'))\n",
    "            self.tar_folder = tar_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OBIS-USA Item record count: : 139\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query ScienceBase API for OBIS-USA parent record information\n",
    "#\n",
    "sb_url = 'https://www.sciencebase.gov/catalog/items?parentId=579b64c6e4b0589fa1c98118&max=1000&format=json&fields=id,title,'\n",
    "OBIS_sbinfo = requests.get(sb_url).json()\n",
    "total_items = len(OBIS_sbinfo['items'])\n",
    "print('{}: {}\\n'.format('OBIS-USA Item record count: ', total_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Archive: 1\n",
      "\tNMFS-COPEPOD: The Global Plankton Database, WEBSEC Sub-Collection\n",
      "\thttps://www.sciencebase.gov/catalog/item/570e9a18e4b0ef3b7ca253aa\n",
      "\tArchive completed for ScienceBase item: : 570e9a18e4b0ef3b7ca253aa\n",
      "Creating Archive: 2\n",
      "\tUSGS South Florida Fish and Invertebrate Assessment Network- Harvest\n",
      "\thttps://www.sciencebase.gov/catalog/item/53a1cc5fe4b0403a441545a5\n",
      "\tArchive completed for ScienceBase item: : 53a1cc5fe4b0403a441545a5\n",
      "Creating Archive: 3\n",
      "\tNational Museum of Natural History - Invertebrate Zoology\n",
      "\thttps://www.sciencebase.gov/catalog/item/570d64fee4b0ef3b7ca14e33\n",
      "\tArchive completed for ScienceBase item: : 570d64fee4b0ef3b7ca14e33\n",
      "\n",
      " *** Archiving complete ***\n"
     ]
    }
   ],
   "source": [
    "# TEST: Archive multiple ScienceBase items (associated files + item information) in OBIS-USA collection\n",
    "#\n",
    "\n",
    "# Customize default BagIt (archive) metadata for OBIS-USA collection\n",
    "\n",
    "aux_dict = {}\n",
    "aux_dict['archive_meta'] = collections.OrderedDict([\n",
    "        ('Archive-Tag-Name', 'OBIS_USA Archive'),\n",
    "        ('Source-Agency-Contact-Name', 'Abby Benson'),\n",
    "        ('Source-Agency-Contact-Email', 'abenson@usgs.gov'),\n",
    "        ('Source-Agency-Contact-Phone', '303.202.4087'),\n",
    "    ])\n",
    "\n",
    "\n",
    "# For testing - limit number of ScienceBase records (items) to process. \n",
    "# switch for loop (below) or set to ~1e6 to process all archives in parent collection\n",
    "max_rec = 3\n",
    "\n",
    "\n",
    "# Iterate through ScienceBase items, Create archives for each \n",
    "#\n",
    "#for indx, sbitem in enumerate(OBIS_sbinfo['items']):\n",
    "for indx, sbitem in enumerate(OBIS_sbinfo['items'][0:min(max_rec, total_items)]):\n",
    "    filename = 'ScienceBase_Archive_' + self.sbitem['id'] + '.tgz'\n",
    "    tarpath = os.path.join(os.path.join(os.getcwd(),'Archives'), filename) \n",
    "    if os.path.exists(tarpath): \n",
    "        print('{}) {}\\n\\t{}\\n\\t{}'.format('Ignoring Archive (exists):', indx+1, \n",
    "                                          sbitem['title'][0:80], sbitem['link']['url']))\n",
    "    else:\n",
    "        print('{} {}\\n\\t{}\\n\\t{}'.format('Creating Archive:', indx+1,  \n",
    "                                          sbitem['title'][0:80], sbitem['link']['url']))\n",
    "        url = str(sbitem['link']['url']) + '?format=json' \n",
    "        sb_item = requests.get(url).json()\n",
    "        archive_func = archive_options(sbitem, custom_meta = aux_dict)\n",
    "        BagIt_Archive = archive_sbitem(sb_item, **archive_func)\n",
    "        \n",
    "print(\"\\n *** Archiving complete ***\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Send Bagit archives(s) somewhere in the world .....\n",
    "\n",
    "# do something amazing here ...\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:TEST]",
   "language": "python",
   "name": "conda-env-TEST-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
